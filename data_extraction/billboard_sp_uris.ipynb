{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Billboard Hot 100s and Spotify URIs\n",
    "\n",
    "The Python package billboard.py doesn't allow extraction of year-end Hot 100 lists and Billboard now requires a pro subscription for their Hot 100 Year End lists. To bypass this, I used Beautiful Soup to scrape each Billboard Hot 100 between 2000 and 2024 from Wikipedia then manually filled in the few missing values due to the unique HTML structure of the charts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install billboard.py\\n!pip install spotipy\\n!pip install lyricsgenius\\n!pip install tqdm\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!pip install billboard.py\n",
    "!pip install spotipy\n",
    "!pip install lyricsgenius\n",
    "!pip install tqdm\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# billboard rankings\n",
    "import billboard\n",
    "\n",
    "# for web scraping the billboard rankings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "# for genius collaborators\n",
    "import lyricsgenius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Billboard Hot 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 99 songs scraped for 2000 (expected 100)\n",
      "Loaded 2000 (99 songs)\n",
      "Loaded 2001 (100 songs)\n",
      "Loaded 2002 (100 songs)\n",
      "Loaded 2003 (100 songs)\n",
      "Loaded 2004 (100 songs)\n",
      "Loaded 2005 (100 songs)\n",
      "Loaded 2006 (100 songs)\n",
      "Loaded 2007 (100 songs)\n",
      "Warning: 98 songs scraped for 2008 (expected 100)\n",
      "Loaded 2008 (98 songs)\n",
      "Loaded 2009 (100 songs)\n",
      "Loaded 2010 (100 songs)\n",
      "Loaded 2011 (100 songs)\n",
      "Warning: 99 songs scraped for 2012 (expected 100)\n",
      "Loaded 2012 (99 songs)\n",
      "Warning: 99 songs scraped for 2013 (expected 100)\n",
      "Loaded 2013 (99 songs)\n",
      "Loaded 2014 (100 songs)\n",
      "Warning: 99 songs scraped for 2015 (expected 100)\n",
      "Loaded 2015 (99 songs)\n",
      "Warning: 98 songs scraped for 2016 (expected 100)\n",
      "Loaded 2016 (98 songs)\n",
      "Loaded 2017 (100 songs)\n",
      "Loaded 2018 (100 songs)\n",
      "Loaded 2019 (100 songs)\n",
      "Loaded 2020 (100 songs)\n",
      "Loaded 2021 (100 songs)\n",
      "Loaded 2022 (100 songs)\n",
      "Loaded 2023 (100 songs)\n",
      "Warning: 99 songs scraped for 2024 (expected 100)\n",
      "Loaded 2024 (99 songs)\n"
     ]
    }
   ],
   "source": [
    "# define the range\n",
    "years = range(2000, 2025)\n",
    "# create an empty list for the songs\n",
    "bb_100 = []\n",
    "\n",
    "# set headers to mimic a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/115.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# loop through each year's Wikipedia page\n",
    "for year in years:\n",
    "    # base url\n",
    "    url = f'https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_{year}'\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f'Failed {year}: Status {response.status_code}')\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', {'class': 'wikitable'})\n",
    "        if not table:\n",
    "            print(f'No table found for {year}')\n",
    "            continue\n",
    "\n",
    "        rows = table.find_all('tr')\n",
    "        year_songs = 0\n",
    "\n",
    "        for row in rows[1:]:  # skip header\n",
    "            cols = row.find_all(['td', 'th'])\n",
    "            if len(cols) < 3:\n",
    "                continue\n",
    "\n",
    "            # get text from any nested tags\n",
    "            rank = ' '.join([x.get_text(strip=True) for x in cols[0].find_all(string=True)]).strip()\n",
    "            title = ' '.join([x.get_text(strip=True) for x in cols[1].find_all(string=True)]).strip()\n",
    "            artist = ' '.join([x.get_text(strip=True) for x in cols[2].find_all(string=True)]).strip()\n",
    "\n",
    "            if not rank or not title or not artist:\n",
    "                continue\n",
    "\n",
    "            bb_100.append({\n",
    "                'year': year,\n",
    "                'rank': rank,\n",
    "                'title': title,\n",
    "                'artist': artist\n",
    "            })\n",
    "            year_songs += 1\n",
    "\n",
    "        # if fewer than 100 songs scraped, warn\n",
    "        if year_songs < 100:\n",
    "            print(f'Warning: {year_songs} songs scraped for {year} (expected 100)')\n",
    "\n",
    "        print(f'Loaded {year} ({year_songs} songs)')\n",
    "        time.sleep(1)  # polite delay\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Failed {year}: {e}')\n",
    "\n",
    "# convert to DataFrame\n",
    "bb_100 = pd.DataFrame(bb_100)\n",
    "\n",
    "# clean up duplicates and whitespace\n",
    "bb_100['title'] = bb_100['title'].str.strip()\n",
    "bb_100['artist'] = bb_100['artist'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Missing\n",
    "\n",
    "There were a total of 9 missing tracks. I manually located these on the respective Wikipedia pages and added them to the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2000 is missing ranks: [23]\n",
      "Year 2008 is missing ranks: [10, 17]\n",
      "Year 2012 is missing ranks: [17]\n",
      "Year 2013 is missing ranks: [18]\n",
      "Year 2015 is missing ranks: [10]\n",
      "Year 2016 is missing ranks: [2, 21]\n",
      "Year 2024 is missing ranks: [20]\n"
     ]
    }
   ],
   "source": [
    "# set the rank column to numeric\n",
    "bb_100['rank'] = pd.to_numeric(bb_100['rank'], errors='coerce')\n",
    "\n",
    "# loop through each year\n",
    "for year in bb_100['year'].unique():\n",
    "    ranks = bb_100[bb_100['year'] == year]['rank'].dropna().astype(int)\n",
    "    missing = set(range(1, 101)) - set(ranks)\n",
    "    if missing:\n",
    "        print(f'Year {year} is missing ranks: {sorted(missing)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dictionary of the missing years and associated song information\n",
    "bb_missing = {'year': [2000,2008,2008,2012,2013,2015,2016,2016,2024]\n",
    "            , 'rank': [23,10,17,17,18,10,2,21,20]\n",
    "            , 'title': ['I Need to Know','Forever','Don\\'t Stop the Music','Whistle','Wrecking Ball','The Hills'\n",
    "                        ,'Sory','Heathens','Snooze']\n",
    "            , 'artist': ['Marc Anthony','Chris Brown','Rihanna','Flo Rida','Miley Cyrus','The Weekend'\n",
    "                         ,'Justin Bieber','Twenty One Pilots','SZA'] }\n",
    "\n",
    "# make into dataframe\n",
    "bb_missing = pd.DataFrame(bb_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 4)\n"
     ]
    }
   ],
   "source": [
    "# combine the two dataframes\n",
    "bb_all = pd.concat([bb_100, bb_missing], ignore_index=True)\n",
    "\n",
    "print(bb_all.shape)   # new row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "bb_all.to_csv('bb_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the 2025 Recent Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = billboard.ChartData('hot-100')  # most recent ranking\n",
    "\n",
    "bb_2025 = []\n",
    "for song in chart:\n",
    "    bb_2025.append({\n",
    "        'Rank': song.rank,\n",
    "        'Title': song.title,\n",
    "        'Artist': song.artist,\n",
    "        'Last Week': song.lastPos,\n",
    "        'Peak Position': song.peakPos,\n",
    "        'Weeks on Chart': song.weeks\n",
    "    })\n",
    "\n",
    "bb_2025 = pd.DataFrame(bb_2025)\n",
    "bb_2025.to_csv('bb_2025.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
